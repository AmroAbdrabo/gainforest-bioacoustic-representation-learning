{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe83bcec-5c08-43c0-892f-601956a810bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "from scipy.signal import butter, lfilter\n",
    "import ast  # Import ast module to use literal_eval\n",
    "from urllib.parse import unquote\n",
    "import torch\n",
    "import evaluate\n",
    "import accelerate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import noisereduce as nr\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import AutoModelForAudioClassification, AutoConfig, ASTFeatureExtractor\n",
    "from typing import Optional, List\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from collections import defaultdict\n",
    "from cycler import cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d837af2d-547e-4010-87cf-8e794c7330d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu121\n",
      "Is CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a99d441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae649e15-1a08-47b9-af03-a2d959f7d143",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DRIVE_FOLDER = \".\" #\"/content/drive/MyDrive/Colab Notebooks\"\n",
    "KEEP_COLS = ['category_number', 'common_name', 'audio_length', 'type', 'remarks', 'quality', 'scientific_name', 'mp3_link', 'region']\n",
    "\n",
    "class Config:\n",
    "    dataset_dir = f\"{DRIVE_FOLDER}/Audio_XenoCanto\"\n",
    "    labels_list = f\"{DRIVE_FOLDER}/xeno_labels.csv\"\n",
    "    model_name = \"ast_baseline\"\n",
    "    backbone_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    n_classes = 800 # number of classes in the dataset\n",
    "    audio_sr = 16000 #Hz\n",
    "    segment_length = 10  #s\n",
    "    fft_window = 0.025 #s\n",
    "    hop_window_length = 0.01 #s\n",
    "    n_mels = 128\n",
    "    low_cut = 1000 #Hz\n",
    "    high_cut = 8000 #Hz\n",
    "    top_db = 100\n",
    "    batch_size = 4 \n",
    "    num_workers = 0\n",
    "    n_splits = 5\n",
    "    log_dir = f\"{DRIVE_FOLDER}/training_logs\"\n",
    "    max_lr = 1e-5\n",
    "    epochs = 5\n",
    "    weight_decay = 0.01\n",
    "    lr_final_div = 1000\n",
    "    amp = True\n",
    "    grad_accum_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "    print_epoch_freq = 1\n",
    "    print_freq = 200\n",
    "    random_seed = 2046\n",
    "    \n",
    "    @classmethod\n",
    "    def copy(cls):\n",
    "        new_class = type('CustomConfig', (cls,), {k: v for k, v in cls.__dict__.items() if not k.startswith('__') and not callable(v)})\n",
    "        return new_class\n",
    "    \n",
    "config = Config.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985e44f-7c71-4cb8-a603-c105c5528f83",
   "metadata": {},
   "source": [
    "## Bird Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef5b1f7-b3a1-4ec2-9c74-ffac06a3a8d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13523, 10)\n",
      "Number of classes with less than 2 samples: 72\n",
      "Number of classes in dataset: 728\n",
      "Number of samples: 11171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>category_number</th>\n",
       "      <th>common_name</th>\n",
       "      <th>audio_length</th>\n",
       "      <th>type</th>\n",
       "      <th>remarks</th>\n",
       "      <th>quality</th>\n",
       "      <th>mp3_link</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>region</th>\n",
       "      <th>file_exists</th>\n",
       "      <th>species_id</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/XC228210-Blue-crowned_Manakin_B_9369_0.wav</td>\n",
       "      <td>XC228210</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:20</td>\n",
       "      <td>call</td>\n",
       "      <td>ID certainty 80%. (Archiv. tape 393 side A tra...</td>\n",
       "      <td>B</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/XC228210-Blue-crowned_Manakin_B_9369_1.wav</td>\n",
       "      <td>XC228210</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:20</td>\n",
       "      <td>call</td>\n",
       "      <td>ID certainty 80%. (Archiv. tape 393 side A tra...</td>\n",
       "      <td>B</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/XC200163-PIPCOR03_0.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/XC200163-PIPCOR03_1.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/XC200163-PIPCOR03_2.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name category_number  \\\n",
       "0  data/XC228210-Blue-crowned_Manakin_B_9369_0.wav        XC228210   \n",
       "1  data/XC228210-Blue-crowned_Manakin_B_9369_1.wav        XC228210   \n",
       "2                     data/XC200163-PIPCOR03_0.wav        XC200163   \n",
       "3                     data/XC200163-PIPCOR03_1.wav        XC200163   \n",
       "4                     data/XC200163-PIPCOR03_2.wav        XC200163   \n",
       "\n",
       "            common_name audio_length        type  \\\n",
       "0  Blue-crowned Manakin         0:20        call   \n",
       "1  Blue-crowned Manakin         0:20        call   \n",
       "2  Blue-crowned Manakin         0:42  call, song   \n",
       "3  Blue-crowned Manakin         0:42  call, song   \n",
       "4  Blue-crowned Manakin         0:42  call, song   \n",
       "\n",
       "                                             remarks quality  \\\n",
       "0  ID certainty 80%. (Archiv. tape 393 side A tra...       B   \n",
       "1  ID certainty 80%. (Archiv. tape 393 side A tra...       B   \n",
       "2  left bank of rio Negro - terra firme forest, w...       C   \n",
       "3  left bank of rio Negro - terra firme forest, w...       C   \n",
       "4  left bank of rio Negro - terra firme forest, w...       C   \n",
       "\n",
       "                                            mp3_link       scientific_name  \\\n",
       "0  //xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...  Lepidothrix_coronata   \n",
       "1  //xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...  Lepidothrix_coronata   \n",
       "2  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "3  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "4  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "\n",
       "     region  file_exists  species_id  group_id  \n",
       "0  amazonas         True         329         0  \n",
       "1  amazonas         True         329         0  \n",
       "2  amazonas         True         329         1  \n",
       "3  amazonas         True         329         1  \n",
       "4  amazonas         True         329         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_audio_meta = pd.read_csv(f\"metadata.csv\")\n",
    "print(df_audio_meta.shape)\n",
    "df_audio_meta = df_audio_meta.dropna().reset_index(drop=True)\n",
    "\n",
    "# Filter out files that do not exist\n",
    "df_audio_meta['file_exists'] = df_audio_meta['file_name'].apply(lambda x: os.path.exists(x))\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['file_exists']].reset_index(drop=True)\n",
    "\n",
    "# parse scientific names\n",
    "df_audio_meta['scientific_name'] = df_audio_meta['scientific_name'].apply(lambda x: \"_\".join(x.split(\" \")))\n",
    "\n",
    "# drop species with less than 2 samples\n",
    "class_counts = df_audio_meta['scientific_name'].value_counts()\n",
    "print(f\"Number of classes with less than 2 samples: {len(class_counts[class_counts < 2])}\")\n",
    "\n",
    "df_audio_meta = df_audio_meta[df_audio_meta['scientific_name'].isin(class_counts[class_counts > 1].index)].copy().reset_index(drop=True)\n",
    "\n",
    "# encode scientific names to label ids\n",
    "label_ids_list = df_audio_meta['scientific_name'].unique().tolist()\n",
    "label_ids_list.sort()\n",
    "label_to_id = {label: i for i, label in enumerate(label_ids_list)}\n",
    "df_audio_meta['species_id'] = df_audio_meta['scientific_name'].map(label_to_id)\n",
    "\n",
    "# save the label mapping\n",
    "label_mapping = pd.DataFrame(label_to_id.items(), columns=['scientific_name', 'species_id'])\n",
    "label_mapping.to_csv(f\"new_label_map.csv\", index=False)\n",
    "\n",
    "# drop samples with no labels\n",
    "df_audio_meta.dropna(subset=['species_id'], inplace=True)\n",
    "df_audio_meta.reset_index(drop=True, inplace=True)\n",
    "df_audio_meta['species_id'] = df_audio_meta['species_id'].astype(int)\n",
    "\n",
    "print(f\"Number of classes in dataset: {df_audio_meta['species_id'].nunique()}\")\n",
    "print(f'Number of samples:', len(df_audio_meta))\n",
    "\n",
    "# save the number of classes in the config\n",
    "config.n_classes = df_audio_meta['species_id'].nunique()\n",
    "\n",
    "# encode mp3 links to group ids for 5-folds\n",
    "group_ids = df_audio_meta['mp3_link'].unique().tolist()\n",
    "group_ids_map = {group_id: i for i, group_id in enumerate(group_ids)}\n",
    "df_audio_meta['group_id'] = df_audio_meta['mp3_link'].map(group_ids_map)\n",
    "\n",
    "df_audio_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b3d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_denoise(file_name):\n",
    "    # Load audio file\n",
    "    audio_data, sample_rate = librosa.load(file_name, sr=None)  # sr=None to preserve original sample rate\n",
    "    # Reduce noise\n",
    "    reduced_noise = nr.reduce_noise(y=audio_data, sr=sample_rate)\n",
    "    # Calculate noise\n",
    "    noise = audio_data - reduced_noise\n",
    "    # Return a pandas Series containing the three audio data arrays\n",
    "    return pd.Series([audio_data, reduced_noise, noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640e9d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/noisereduce/spectralgate/nonstationary.py:71: RuntimeWarning: invalid value encountered in divide\n",
      "  sig_mult_above_thresh = (abs_sig_stft - sig_stft_smooth) / sig_stft_smooth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>category_number</th>\n",
       "      <th>common_name</th>\n",
       "      <th>audio_length</th>\n",
       "      <th>type</th>\n",
       "      <th>remarks</th>\n",
       "      <th>quality</th>\n",
       "      <th>mp3_link</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>region</th>\n",
       "      <th>file_exists</th>\n",
       "      <th>species_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>original_audio</th>\n",
       "      <th>denoised_audio</th>\n",
       "      <th>noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/XC228210-Blue-crowned_Manakin_B_9369_0.wav</td>\n",
       "      <td>XC228210</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:20</td>\n",
       "      <td>call</td>\n",
       "      <td>ID certainty 80%. (Archiv. tape 393 side A tra...</td>\n",
       "      <td>B</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.004058838, -0.008453369, 0.020111084, -0.00...</td>\n",
       "      <td>[-0.00055981963, 0.001044453, -0.0008553348, 0...</td>\n",
       "      <td>[0.0046186578, -0.009497822, 0.020966418, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/XC228210-Blue-crowned_Manakin_B_9369_1.wav</td>\n",
       "      <td>XC228210</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:20</td>\n",
       "      <td>call</td>\n",
       "      <td>ID certainty 80%. (Archiv. tape 393 side A tra...</td>\n",
       "      <td>B</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0049438477, 0.0087890625, 0.004058838, 0.0...</td>\n",
       "      <td>[0.0003904775, 0.000830834, 0.00082218484, 0.0...</td>\n",
       "      <td>[-0.005334325, 0.007958229, 0.003236653, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/XC200163-PIPCOR03_0.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>[-3.0517578e-05, 0.0, -3.0517578e-05, 0.0, -3....</td>\n",
       "      <td>[-2.400732e-06, -8.714055e-07, -3.2778255e-06,...</td>\n",
       "      <td>[-2.8116847e-05, 8.714055e-07, -2.7239752e-05,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/XC200163-PIPCOR03_1.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0115356445, -0.007843018, 0.0052490234, 0.0...</td>\n",
       "      <td>[0.0035293808, 0.00043889682, 0.0012755911, 0....</td>\n",
       "      <td>[0.0080062635, -0.0082819145, 0.0039734324, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/XC200163-PIPCOR03_2.wav</td>\n",
       "      <td>XC200163</td>\n",
       "      <td>Blue-crowned Manakin</td>\n",
       "      <td>0:42</td>\n",
       "      <td>call, song</td>\n",
       "      <td>left bank of rio Negro - terra firme forest, w...</td>\n",
       "      <td>C</td>\n",
       "      <td>//xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...</td>\n",
       "      <td>Lepidothrix_coronata</td>\n",
       "      <td>amazonas</td>\n",
       "      <td>True</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.005493164, -0.021057129, -0.010803223, 0.0...</td>\n",
       "      <td>[0.003562114, -0.0011988133, -0.0022794271, -0...</td>\n",
       "      <td>[-0.009055278, -0.019858316, -0.008523796, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name category_number  \\\n",
       "0  data/XC228210-Blue-crowned_Manakin_B_9369_0.wav        XC228210   \n",
       "1  data/XC228210-Blue-crowned_Manakin_B_9369_1.wav        XC228210   \n",
       "2                     data/XC200163-PIPCOR03_0.wav        XC200163   \n",
       "3                     data/XC200163-PIPCOR03_1.wav        XC200163   \n",
       "4                     data/XC200163-PIPCOR03_2.wav        XC200163   \n",
       "\n",
       "            common_name audio_length        type  \\\n",
       "0  Blue-crowned Manakin         0:20        call   \n",
       "1  Blue-crowned Manakin         0:20        call   \n",
       "2  Blue-crowned Manakin         0:42  call, song   \n",
       "3  Blue-crowned Manakin         0:42  call, song   \n",
       "4  Blue-crowned Manakin         0:42  call, song   \n",
       "\n",
       "                                             remarks quality  \\\n",
       "0  ID certainty 80%. (Archiv. tape 393 side A tra...       B   \n",
       "1  ID certainty 80%. (Archiv. tape 393 side A tra...       B   \n",
       "2  left bank of rio Negro - terra firme forest, w...       C   \n",
       "3  left bank of rio Negro - terra firme forest, w...       C   \n",
       "4  left bank of rio Negro - terra firme forest, w...       C   \n",
       "\n",
       "                                            mp3_link       scientific_name  \\\n",
       "0  //xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...  Lepidothrix_coronata   \n",
       "1  //xeno-canto.org/sounds/uploaded/OOECIWCSWV/XC...  Lepidothrix_coronata   \n",
       "2  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "3  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "4  //xeno-canto.org/sounds/uploaded/DGVLLRYDXS/XC...  Lepidothrix_coronata   \n",
       "\n",
       "     region  file_exists  species_id  group_id  \\\n",
       "0  amazonas         True         329         0   \n",
       "1  amazonas         True         329         0   \n",
       "2  amazonas         True         329         1   \n",
       "3  amazonas         True         329         1   \n",
       "4  amazonas         True         329         1   \n",
       "\n",
       "                                      original_audio  \\\n",
       "0  [0.004058838, -0.008453369, 0.020111084, -0.00...   \n",
       "1  [-0.0049438477, 0.0087890625, 0.004058838, 0.0...   \n",
       "2  [-3.0517578e-05, 0.0, -3.0517578e-05, 0.0, -3....   \n",
       "3  [0.0115356445, -0.007843018, 0.0052490234, 0.0...   \n",
       "4  [-0.005493164, -0.021057129, -0.010803223, 0.0...   \n",
       "\n",
       "                                      denoised_audio  \\\n",
       "0  [-0.00055981963, 0.001044453, -0.0008553348, 0...   \n",
       "1  [0.0003904775, 0.000830834, 0.00082218484, 0.0...   \n",
       "2  [-2.400732e-06, -8.714055e-07, -3.2778255e-06,...   \n",
       "3  [0.0035293808, 0.00043889682, 0.0012755911, 0....   \n",
       "4  [0.003562114, -0.0011988133, -0.0022794271, -0...   \n",
       "\n",
       "                                               noise  \n",
       "0  [0.0046186578, -0.009497822, 0.020966418, -0.0...  \n",
       "1  [-0.005334325, 0.007958229, 0.003236653, 0.002...  \n",
       "2  [-2.8116847e-05, 8.714055e-07, -2.7239752e-05,...  \n",
       "3  [0.0080062635, -0.0082819145, 0.0039734324, 0....  \n",
       "4  [-0.009055278, -0.019858316, -0.008523796, 0.0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame with columns for each type of audio data\n",
    "# Apply the function to each row in the DataFrame and expand the results into separate columns\n",
    "df_audio_meta[['original_audio', 'denoised_audio', 'noise']] = df_audio_meta['file_name'].apply(load_and_denoise)\n",
    "df_audio_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3656c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_audio_meta[['original_audio', 'denoised_audio', 'noise', 'species_id', 'group_id']]\n",
    "data_df.to_csv(\"birdset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1cff048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_audio</th>\n",
       "      <th>denoised_audio</th>\n",
       "      <th>noise</th>\n",
       "      <th>species_id</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.004058838, -0.008453369, 0.020111084, -0.00...</td>\n",
       "      <td>[-0.00055981963, 0.001044453, -0.0008553348, 0...</td>\n",
       "      <td>[0.0046186578, -0.009497822, 0.020966418, -0.0...</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0049438477, 0.0087890625, 0.004058838, 0.0...</td>\n",
       "      <td>[0.0003904775, 0.000830834, 0.00082218484, 0.0...</td>\n",
       "      <td>[-0.005334325, 0.007958229, 0.003236653, 0.002...</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-3.0517578e-05, 0.0, -3.0517578e-05, 0.0, -3....</td>\n",
       "      <td>[-2.400732e-06, -8.714055e-07, -3.2778255e-06,...</td>\n",
       "      <td>[-2.8116847e-05, 8.714055e-07, -2.7239752e-05,...</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0115356445, -0.007843018, 0.0052490234, 0.0...</td>\n",
       "      <td>[0.0035293808, 0.00043889682, 0.0012755911, 0....</td>\n",
       "      <td>[0.0080062635, -0.0082819145, 0.0039734324, 0....</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.005493164, -0.021057129, -0.010803223, 0.0...</td>\n",
       "      <td>[0.003562114, -0.0011988133, -0.0022794271, -0...</td>\n",
       "      <td>[-0.009055278, -0.019858316, -0.008523796, 0.0...</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      original_audio  \\\n",
       "0  [0.004058838, -0.008453369, 0.020111084, -0.00...   \n",
       "1  [-0.0049438477, 0.0087890625, 0.004058838, 0.0...   \n",
       "2  [-3.0517578e-05, 0.0, -3.0517578e-05, 0.0, -3....   \n",
       "3  [0.0115356445, -0.007843018, 0.0052490234, 0.0...   \n",
       "4  [-0.005493164, -0.021057129, -0.010803223, 0.0...   \n",
       "\n",
       "                                      denoised_audio  \\\n",
       "0  [-0.00055981963, 0.001044453, -0.0008553348, 0...   \n",
       "1  [0.0003904775, 0.000830834, 0.00082218484, 0.0...   \n",
       "2  [-2.400732e-06, -8.714055e-07, -3.2778255e-06,...   \n",
       "3  [0.0035293808, 0.00043889682, 0.0012755911, 0....   \n",
       "4  [0.003562114, -0.0011988133, -0.0022794271, -0...   \n",
       "\n",
       "                                               noise  species_id  group_id  \n",
       "0  [0.0046186578, -0.009497822, 0.020966418, -0.0...         329         0  \n",
       "1  [-0.005334325, 0.007958229, 0.003236653, 0.002...         329         0  \n",
       "2  [-2.8116847e-05, 8.714055e-07, -2.7239752e-05,...         329         1  \n",
       "3  [0.0080062635, -0.0082819145, 0.0039734324, 0....         329         1  \n",
       "4  [-0.009055278, -0.019858316, -0.008523796, 0.0...         329         1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0032da-8e10-4652-95c6-01dbfac47919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False [2093 2094 2095] [2093 2094 2095]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "npres1 = [np.isnan(data_df.iloc[i]['original_audio']).any() for i in range(len(data_df))]\n",
    "npres2 = [np.isnan(data_df.iloc[i]['denoised_audio']).any() for i in range(len(data_df))]\n",
    "npres3 = [np.isnan(data_df.iloc[i]['noise']).any() for i in range(len(data_df))]\n",
    "print(any(npres1),np.where(npres2)[0], np.where(npres3)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d3a0ae-5ec0-4676-bab8-d343b9d7cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop(index=[2093, 2094, 2095])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "101909e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self, w):\n",
    "        self.class_weights = w\n",
    "\n",
    "    def weighted_cross_entropy_with_logits(self, logits, targets):\n",
    "        \"\"\"\n",
    "        This function applies a weighted cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "        logits (torch.Tensor): Logits output from the model (before softmax).\n",
    "        targets (torch.Tensor): Ground truth labels.\n",
    "        class_weights (torch.Tensor): Tensor of weights for each class.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Computed weighted cross-entropy loss.\n",
    "        \"\"\"\n",
    "        return F.cross_entropy(logits.float(), targets, weight=self.class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c783d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCAUCScore:\n",
    "    def __init__(self, average='macro', multi_class='ovo'):\n",
    "        self.num_classes = 728\n",
    "        self.average = average\n",
    "        self.multi_class = multi_class  # 'ovo' (one-vs-one) or 'ovr' (one-vs-rest)\n",
    "        self.label_ids = np.arange(self.num_classes)\n",
    "\n",
    "    def roc_auc_loss(self, logits, targets):\n",
    "        \"\"\"\n",
    "        This function computes the ROC-AUC loss for multi-class classification.\n",
    "\n",
    "        Args:\n",
    "        logits (torch.Tensor): Logits output from the model (before softmax).\n",
    "        targets (torch.Tensor): Ground truth labels.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Computed ROC-AUC loss.\n",
    "        \"\"\"\n",
    "        # Apply softmax to get probabilities\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probas = torch.exp(F.log_softmax(logits, dim=1))\n",
    "\n",
    "        # Detach and move to CPU for sklearn compatibility\n",
    "        #probas = logits\n",
    "        probas = probas.detach().cpu().numpy()\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        df_scores = pd.DataFrame(probas, columns=self.label_ids)\n",
    "        df_scores['target'] = targets\n",
    "        \n",
    "        # remove samples with classes which is predeicted as 0 in all samples\n",
    "        unscored_cols = df_scores.columns[df_scores.sum(axis=0) == 0]\n",
    "        rows_to_remove = df_scores['target'].isin(unscored_cols)\n",
    "        df_scores = df_scores[~rows_to_remove]\n",
    "        \n",
    "        eval_score = roc_auc_score(\n",
    "            y_true=df_scores['target'].values,\n",
    "            y_score=df_scores[self.label_ids].values,\n",
    "            average=self.average, \n",
    "            multi_class=self.multi_class,\n",
    "            labels=self.label_ids\n",
    "        )\n",
    "        \n",
    "        # Convert the evaluation score back to a PyTorch tensor and move it to the same device as logits\n",
    "        #eval_score_tensor = torch.tensor(eval_score, dtype=torch.float32, device=logits.device)\n",
    "\n",
    "        return eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = np.unique(data_df[['species_id']])\n",
    "\n",
    "# Calculate class weights using the 'balanced' option, which automatically adjusts for class imbalance.\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=data_df['species_id'])\n",
    "\n",
    "# Create a dictionary mapping each class to its respective class weight.\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Print the computed class weights to the console.\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36b272ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.755672\n"
     ]
    }
   ],
   "source": [
    "# Specify the pre-trained model you want to use.\n",
    "model_str = \"dima806/bird_sounds_classification\" #\"facebook/wav2vec2-base-960h\"\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, pipeline, TrainingArguments, Trainer\n",
    "\n",
    "# Create an instance of the feature extractor for audio.\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "489f2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE_HZ = 16000\n",
    "\n",
    "# Define the maximum audio interval length to consider in seconds\n",
    "MAX_SECONDS = 10\n",
    "\n",
    "# Calculate the maximum audio interval length in samples by multiplying the rate and seconds\n",
    "MAX_LENGTH = RATE_HZ * MAX_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0084c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        self.df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio = np.array(row['denoised_audio'], dtype=np.float32)\n",
    "        if np.random.rand() < 0.5:\n",
    "            noise = np.array(self.df.sample(n=1).iloc[0]['noise'], dtype=np.float32)\n",
    "            audio += noise\n",
    "        processed = feature_extractor(audio, sampling_rate=RATE_HZ, max_length=MAX_LENGTH, truncation=True, return_tensors=\"pt\")\n",
    "        return processed.input_values.squeeze(0), torch.tensor(row['species_id'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e22500ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class BirdSongClassifier(pl.LightningModule):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained(\"dima806/bird_sounds_classification\")\n",
    "        config.num_labels = 728\n",
    "        self.model = AutoModelForAudioClassification.from_config(config)\n",
    "        # Convert class weights from dict to tensor and register as model parameter\n",
    "        # self.register_buffer('class_weights', )\n",
    "        weights = torch.tensor([class_weights[i] for i in sorted(class_weights.keys())]).float()\n",
    "        weights = weights.to('cuda')\n",
    "        self.loss_fn = CrossEntropyLoss(weights)\n",
    "        self.eval_fn  = ROCAUCScore()\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=3e-6, weight_decay=0.02)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x).logits  # Get logits from model\n",
    "        loss = self.loss_fn.weighted_cross_entropy_with_logits(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.model(x).logits\n",
    "        loss = self.eval_fn.roc_auc_loss(logits, y)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.validation_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.training_step_outputs).mean()\n",
    "        self.log(\"training_epoch_average\", epoch_average)\n",
    "        self.training_losses.append(epoch_average)\n",
    "        self.training_step_outputs.clear()  # free memory\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_losses.append(epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d44041fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "288368eb-2978-4127-8243-534fc0cf0977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11168, 5)\n",
      "(11168, 5)\n"
     ]
    }
   ],
   "source": [
    "shuffled_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75e61f7c-d21a-43d0-b240-c635d17d50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Get the group ids\n",
    "groups = data_df['group_id'].values\n",
    "\n",
    "# Split the data using GroupKFold\n",
    "folds = list(group_kfold.split(shuffled_df, groups=groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9fa85cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSongDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_df, train_idx, valid_idx, batch_size=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_df = data_df #BirdSongDataset(data_df)\n",
    "        #self.fold = fold #  from 0 to 4\n",
    "        self.tot_len = len(data_df)\n",
    "        self.fold_size = int(len(data_df)*0.2)\n",
    "        self.train_idx = train_idx\n",
    "        self.valid_idx = valid_idx\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # train_size = int(0.8 * len(self.data_df))\n",
    "        # val_size = len(self.data_df) - train_size\n",
    "        # self.train_dataset, self.val_dataset = random_split(self.dataset, [train_size, val_size])\n",
    "        # use GroupShuffleSplit to ensure that samples from the same group are not split between train and val\n",
    "        # start = self.fold * self.fold_size\n",
    "        # end =   (self.fold + 1)*self.fold_size if self.fold < 4 else self.tot_len\n",
    "        # valid_idx = np.arange(start, end)\n",
    "        # train_idx = [el for el in np.arange(0, self.tot_len) if el not in np.arange(start, end)]  #next(GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42).split(self.data_df, groups=self.data_df['group_id']))\n",
    "        self.train_dataset = BirdSongDataset(self.data_df.loc[self.train_idx])\n",
    "        self.valid_dataset = BirdSongDataset(self.data_df.loc[self.valid_idx])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=10, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=10, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bab2dbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RangeIndex(start=0, stop=11168, step=1),\n",
       " RangeIndex(start=0, stop=11168, step=1))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.index, shuffled_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43e88453",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30db2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard Logger\n",
    "logger = [ pl.loggers.TensorBoardLogger(save_dir=f'tb_logs{i}/', name='birdsong_classifier') for i in range(5)]\n",
    "\n",
    "# Model Checkpoint based on validation loss improvement\n",
    "checkpoint_callbacks = [pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f'model_checkpoints{i}',\n",
    "    filename='best-birdsong',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='max'\n",
    ") for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c6b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /workspace/model_checkpoints0 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                              | Params\n",
      "------------------------------------------------------------\n",
      "0 | model | Wav2Vec2ForSequenceClassification | 94.8 M\n",
      "------------------------------------------------------------\n",
      "94.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "94.8 M    Total params\n",
      "379.023   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d236b97a844bd5a957e54776e62923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2234: 'val_loss' reached 0.55118 (best 0.55118), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 4468: 'val_loss' reached 0.58371 (best 0.58371), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    \n",
      "if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
      ": can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    \n",
      "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():    \n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa0757cd900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
      "AssertionErrorcan only test a child process\n",
      "Epoch 2, global step 6702: 'val_loss' reached 0.61534 (best 0.61534), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 8936: 'val_loss' reached 0.65712 (best 0.65712), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 11170: 'val_loss' reached 0.67875 (best 0.67875), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 13404: 'val_loss' reached 0.70397 (best 0.70397), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 15638: 'val_loss' was not in top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 17872: 'val_loss' reached 0.71964 (best 0.71964), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 20106: 'val_loss' reached 0.73694 (best 0.73694), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 22340: 'val_loss' reached 0.73754 (best 0.73754), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 24574: 'val_loss' reached 0.75142 (best 0.75142), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 26808: 'val_loss' reached 0.76186 (best 0.76186), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 29042: 'val_loss' reached 0.77022 (best 0.77022), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 31276: 'val_loss' reached 0.77246 (best 0.77246), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 33510: 'val_loss' reached 0.78648 (best 0.78648), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 35744: 'val_loss' was not in top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 37978: 'val_loss' reached 0.78708 (best 0.78708), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 40212: 'val_loss' reached 0.79260 (best 0.79260), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 42446: 'val_loss' reached 0.80021 (best 0.80021), saving model to '/workspace/model_checkpoints0/best-birdsong-v2.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 44680: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs1/birdsong_classifier\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /workspace/model_checkpoints1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                              | Params\n",
      "------------------------------------------------------------\n",
      "0 | model | Wav2Vec2ForSequenceClassification | 94.8 M\n",
      "------------------------------------------------------------\n",
      "94.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "94.8 M    Total params\n",
      "379.023   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867983eb06db4d75a8a535281e060d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2234: 'val_loss' reached 0.56267 (best 0.56267), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 4468: 'val_loss' reached 0.59460 (best 0.59460), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 6702: 'val_loss' reached 0.63906 (best 0.63906), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 8936: 'val_loss' reached 0.66846 (best 0.66846), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 11170: 'val_loss' reached 0.69546 (best 0.69546), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 13404: 'val_loss' reached 0.70322 (best 0.70322), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 15638: 'val_loss' reached 0.72187 (best 0.72187), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 17872: 'val_loss' reached 0.73426 (best 0.73426), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 20106: 'val_loss' reached 0.74694 (best 0.74694), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 22340: 'val_loss' was not in top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 24574: 'val_loss' reached 0.75918 (best 0.75918), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 26808: 'val_loss' reached 0.77499 (best 0.77499), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 29042: 'val_loss' reached 0.77932 (best 0.77932), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 31276: 'val_loss' reached 0.78290 (best 0.78290), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 33510: 'val_loss' reached 0.78812 (best 0.78812), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 35744: 'val_loss' reached 0.79141 (best 0.79141), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 37978: 'val_loss' reached 0.79857 (best 0.79857), saving model to '/workspace/model_checkpoints1/best-birdsong-v1.ckpt' as top 1\n",
      "/tmp/ipykernel_158/2558313378.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.training_step_outputs.append(torch.tensor(loss, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "for i, (train_idx, test_idx) in enumerate(folds):\n",
    "    model = BirdSongClassifier(class_weights) # BirdSongClassifier.load_from_checkpoint('./model_checkpoints/best-birdsong.ckpt', class_weights = class_weights) \n",
    "    data_module = BirdSongDataModule(shuffled_df, train_idx, test_idx)\n",
    "    trainer = pl.Trainer(max_epochs=20, callbacks=[checkpoint_callbacks[i]], logger=logger[i])\n",
    "    trainer.fit(model, data_module)\n",
    "    \n",
    "    with open(f'tr_loss{i}.txt', 'w') as file:\n",
    "        for item in model.training_losses:\n",
    "            file.write(f\"{item} \")\n",
    "            \n",
    "    with open(f'val_loss{i}.txt', 'w') as file:\n",
    "        for item in model.validation_losses:\n",
    "            file.write(f\"{item} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
